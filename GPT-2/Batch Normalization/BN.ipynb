{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 2.2334\n",
      "  10000/ 200000: 1.9733\n",
      "  20000/ 200000: 2.4639\n",
      "  30000/ 200000: 2.0926\n",
      "  40000/ 200000: 2.4991\n",
      "  50000/ 200000: 2.3908\n",
      "  60000/ 200000: 2.2974\n",
      "  70000/ 200000: 2.6379\n",
      "  80000/ 200000: 2.2312\n",
      "  90000/ 200000: 1.7241\n",
      " 100000/ 200000: 2.4090\n",
      " 110000/ 200000: 1.9261\n",
      " 120000/ 200000: 1.9298\n",
      " 130000/ 200000: 2.1434\n",
      " 140000/ 200000: 2.4266\n",
      " 150000/ 200000: 2.2136\n",
      " 160000/ 200000: 1.4396\n",
      " 170000/ 200000: 2.0837\n",
      " 180000/ 200000: 2.5630\n",
      " 190000/ 200000: 1.8784\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\t# minibatch construct\n",
    "\tix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "\tXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\t\n",
    "\t# forward pass\n",
    "\temb = C[Xb] # embed the characters into vectors\n",
    "\tembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\t# Linear layer\n",
    "\thpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "\t# BatchNorm layer\n",
    "\t# -------------------------------------------------------------\n",
    "\tbnmeani = hpreact.mean(0, keepdim=True)\n",
    "\tbnstdi = hpreact.std(0, keepdim=True)\n",
    "\thpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "\twith torch.no_grad():\n",
    "\t\tbnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "\t\tbnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\t# -------------------------------------------------------------\n",
    "\t# Non-linearity\n",
    "\th = torch.tanh(hpreact) # hidden layer\n",
    "\tlogits = h @ W2 + b2 # output layer\n",
    "\tloss = F.cross_entropy(logits, Yb) # loss function\n",
    "\t\n",
    "\t# backward pass\n",
    "\tfor p in parameters:\n",
    "\t\tp.grad = None\n",
    "\tloss.backward()\n",
    "\t\n",
    "\t# update\n",
    "\tlr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "\tfor p in parameters:\n",
    "\t\tp.data += -lr * p.grad\n",
    "\n",
    "\t# track stats\n",
    "\tif i % 10000 == 0: # print every once in a while\n",
    "\t\tprint(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\tlossi.append(loss.log10().item())\n",
    "\t\n",
    "\t#break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x709a2e1147f0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUp5JREFUeJzt3Xl4U1X6B/Bv0r1AW6C0pVAo+04LhZYim1oBdRR3QAWsioo66tQVF3CdovADZxwGFFlUHEFnwBVBKVS2shXKDrKXpQsFulDomvP7AxqaNsu9yU3uTfr98PR5aHJz77lJmvvmnPe8RyeEECAiIiLSCL3aDSAiIiKqjcEJERERaQqDEyIiItIUBidERESkKQxOiIiISFMYnBAREZGmMDghIiIiTWFwQkRERJrirXYDpDAYDDh79iyaNGkCnU6ndnOIiIhIAiEESkpKEBkZCb1een+IWwQnZ8+eRVRUlNrNICIiIjucOnUKrVu3lry9WwQnTZo0AXD15IKCglRuDREREUlRXFyMqKgo43VcKrcITmqGcoKCghicEBERuRm5KRlMiCUiIiJNYXBCREREmsLghIiIiDSFwQkRERFpCoMTIiIi0hQGJ0RERKQpDE6IiIhIUxicEBERkaYwOCEiIiJNYXBCREREmsLghIiIiDSFwQkRERFpCoMTF9p1qhCLNh6HwSDUbgqRLEIIHMgpxuWKKrWbQkQNgFusSuwpRs3eCABo1tgPd8ZEqtwaIunSDuTj8S+3o0OLRkh7cZjazSEiD8eeExUcyStRuwlEsizPOgMAOHquVOWWEFFDwOBEonMl5RCCwzFERETOxuBEgrUH89H/g9V4fkmW2k0hIiLyeAxOJPhkzWEAwI+7zqrcEiIiIs/H4MQCDuEQERGpg8GJGb/uyUG/91cj4+h5tZtCRETU4DA4MWPS1ztwvrQC4xdsUbspREREDY5dwcns2bMRHR0Nf39/JCQkYOvWrRa3XbRoEXQ6ncmPv7+/3Q12pWpnFUvT6ZyzX4U57fyJiIiskB2cLF26FCkpKZg6dSp27NiBmJgYjBgxAvn5+RYfExQUhJycHOPPyZMnHWo0Od+KPTno+tav+HVPjtpNISI3wMrXpCTZwcnMmTMxceJEJCcno3v37pg7dy4CAwOxYMECi4/R6XSIiIgw/oSHhzvUaHK+p7/egcpqgUlf71C7KUSkcTlFV9D3/d+R+usBtZtCNsz87RCWbM1Wuxk2yQpOKioqkJmZiaSkpOs70OuRlJSEjIwMi4+7dOkS2rZti6ioKIwaNQr79u2zepzy8nIUFxeb/BARkTbNXnsEhZcr8ekfx9RuClmx90wR/rnmCF5btkftptgkKzgpKChAdXV1vZ6P8PBw5Obmmn1Mly5dsGDBAvzwww9YvHgxDAYDBg4ciNOnT1s8TmpqKoKDg40/UVFRcpqpuNqdlWkH8lRrh6toeRr1t9tOYdLiTJRVVjtl/5fKq3DPvzfi0z+OOmX/bku7bwkikqjoSqXaTZDM6bN1EhMTMX78eMTGxmLo0KFYtmwZWrRogU8//dTiYyZPnoyioiLjz6lTp5zdTMke+2K72k1wqu93nkH/D1ar3QyLXvnfbvy6Nxf/2eKcbskvM05gR3YhUn896JT9ExGRbbJWJQ4NDYWXlxfy8kx7D/Ly8hARESFpHz4+PujTpw+OHDlicRs/Pz/4+fnJaZpb+jOvBFcqqhETFaJ2U4xeWJqldhMksfYN4HhBKf615ggmDeuAjmGNZe23rNLgaNOIiMhBsnpOfH19ERcXh7S0NONtBoMBaWlpSExMlLSP6upq7NmzBy1btpTXUg9SM5F4+Kx1GDV7Iy6UVqjaHi2pNgj8bWkWvth0wu59PPz5Fvxvx2mM+Wyzcg0jIiKXkT2sk5KSgnnz5uGLL77AgQMHMGnSJJSWliI5ORkAMH78eEyePNm4/bvvvovffvsNx44dw44dO/Dwww/j5MmTePzxx5U7CycxCOCbrdlwdgpGblGZcw/gRn7bl4vlO89g6o/7cKbwil37qHlcwaVyJZumGYs3n8T4BVtxuaJK7aYQETmF7OBk9OjRmDFjBqZMmYLY2FhkZWVh5cqVxiTZ7Oxs5ORcr41x8eJFTJw4Ed26dcNtt92G4uJibNq0Cd27d1fuLJxosoazmjNPXqyXoLvrVCH+syXbaUmt1QaB/GLnBVMlZdcvuG8u1+5zr6Y3v9+LdX+ewyIHepeIiLRMVs5JjWeffRbPPvus2fvS09NNfp81axZmzZplz2HIhnvnbAIArH/lRkQ1CwQAjJq9EQAQ1sQPSd2VryfzxJfbkXYwH4sfS8CgTqGK77+2Qit5JZw8AlwqY88JEXkmrq3jAXLMDAv9mV/ilGOlHbxaCXjhxuNO2b+nO33xMvacLrK6jcEgcJF5SORGdHCPJTnIfTA4kYDf0kkpgz5cizv+tQHZ5y9b3ObxL7ejz3u/Y2f2RRe2jFxJCIGPVh7EN25QqdNTVVUbsOZgHoouu0/tD0fNSXef+k0MTjTuSoVzio1pSVllNR7/Yju+3qLumktpB/Lwz7TDLjnWgVzLVY/XXOud+jLDc9egqjaIBp3Qm3WqEP9OP6rpnDZPNyf9KB5dtB2jP7Nc3dzTbDhSoHYTJGNwogKpixJnnryAblNW4r2f9wMAVu/PQ8LfV2PTUfd5g0mxZGs2Vh/IwxvL97rsmL/uyakXDHl6gT0tue0f69F9yioUXm6Yw1fuVKnTU32fdQYAcDDXOUPg5BgGJyqoOz4rLAwcffjrIQDA/A1X8zse/3I78orL8eC8LTaPUXSlEhO/3I6Ve80vK/DPtMN496f9cpptU1llNZ78aju+3S6vom/tGTqWngslCXF1QcM3lu/FiYJSpx+P6juUd/WCkHH0vMotISVI/cJFJJVds3VI+2oW4Pp9fx5OTLu93v0zf/8TAHB/v9bo1jJIkWMu3nwSq/blYdW+PNwZEwl/Hy9F9muWnVOlc4quYNS/Nhp/tzYjiIiI1MGeEw/gSE2TCQu22vW4ajPHLKyVWPbyf3cb/6+lvJlZv/+J/BL1i7NpeG1Fs1zRo0XkTJknLyBlaRbOaeDvn2xjcKISV485F5eZP569F+r0Q+es3v/TrrMAgEUbj6PblJX48drvwNW1b975aZ8qlXHVCArSD+Vj/WHzz1f2+cuYvfaIXe8HrYcLBoPAthMXVE18vVhageMeOHRXbRA4a2cF5Ybq3jkZWLbzDN76XtncNiEEVu3LxakLlmfgkXwc1lHJbf9Y77JjzV57BNNXHcKUv3SHv4+X7AvhiYJSRIc2suvYb1/La3num524MyYSAHDPvzfi4uVK7DpViGVP32DXft1FSVklHlm4DQBw8L2R9e7/yyfrUVxWhUO5Jfjn2D4m9zmryq+rLNx0Au/9vB/92jbFfycNVKUNfd77HYBpoUJP8ORX27H6QD7mT+iHm7spX2xRLndKOTlxXtlg9bf9eXjyq8yr+zYzhE72YXAihcIXCZ0OktaNUaorffqqq4m17/5sXwLssBnpSL2nF8bGt1GkPRevDf9knSqsd5+bX4/rKS2/PqRVUV1/xePia8nAW457XmLokms1PLafVL9ey47six4VnKw+cHW6+fwNxzURnDRk245fULsJHonDOhK46nqZvHArol/7xeXdg//LPG1zm9lrj7igJdJpPYY5nFeCk1a/oWn9DK6qrDYg61Qhqg3u0V4i8gwNPjgpKavEU19l4pfdObY3dhIhgJ3ZF7H2Wh7HWz+4rt4HALz43S6bAdHpi1dQZeabvzO5a+XGoiuVuGXWOgydnm5xm+0n1O9NkOKt7/firtkbsWrf9QUmz3voas9EpB0NPjh556f9WLkvF8/8Z4fFbc5fUrZQVM003tqyawUHchd0U+I77Ukr5dRr1FQutdwO+1pSu0ZC7T0ck5nIaDAIlwdQ5uRJWLX58w3HceOMdOc3xkFLttWvWRP3/mqcunAZZZXVqKhy7Pme9PUOi8nCcnyZcUKR/VDD8M5P+3D0nOclSnuSBh+c/FfCkIaU/BBrDAaBI/mXLN5fbRB4pdbUW62qrNZu174QArd/ssFqbwVQv1iUtYvr2M82448/lbvg1c2nccYskv9mnsbYzzaj8HIFqg0CBU7q5fhlTw56TF2F/h+sdjhxd9z8rfjhWrVOe2w/cQFTftiHcfPtmxavBu3+Jbmf8qpqjJu/RfLQ88HcEizceELWMf6x+jBumpHOBTldqEEHJ66aDfH68j1ImvmHxftHzd6IchvfQI+dsxzcaNXkZfIDrko7ez6qDQIHcoplB5IPfJphcUgr49h5u+vA1LjhwzV2P1buu7O8qhovfbcLGcfO4x9ph/HIwq3o9/5qZMpMSP1ldw6SF261+kF86sJlVBsEiq5UKpLE/PySLLsf6+iXB3KcTsUSsT/tysH6wwXGxH9nmLX6TxwrKMXnG4457RhkqkEHJ8t3mn5bW7XPfKl3R5nrGrem5rO+9of+Tf9nObjJPHlRk8WFvtkq77wBYMoP+5zQEutu/+d6/G1pllP2XTuRdNkO2710jvhmy/UVbhduPIH1h6+uwfT15voLCF4orbDYa/TMf3Zg7aFzeG7JTuc0lEhBZZWuK/JYxcRwl2nQwclXdT60n/wqUxM5C+am2FozfdUhLNt5Bq8v1+4Kp+a+WDnybavut3VHPjKKy6rqBaqWjyuQX2Jf8bh3JKxlJKeXo+5zcF5il/Ppi5fR973fMeLjdVa3qwluiMh+M3//E+Pmb7G7V7ihatDByc7swnq3dXzjV9c3pI5qg7Br6qb1qauOM5fwajAIrDmYh3Ml5ZqsUVJSVonNx87DoNA3nmm/HkT8B2nGGh5Kyim6gnvnbFJ8v3Wt3n915s3xglL8ti8XX285ibWHrCc7W1MqowLsn3klxuPL9e32U1hsphfIU6UfysfTX2fiAvMcnM6ZU+X/mXYY6w8X4Hc73/cNFYuwadT/dpxGjgrl3eVauv0UJi/bg+AAHzyUoEyRNnP2ninChiOWv8kbLERG98/NwMHcErw3qgfGJUY73I5P110dc37/lwMYY6EonT1Bmg46nChwffnrJ65VtgSAo3+/DV56+b1Z7/98AB/e1xtrD+bju8xT+OCuXmjayNfstsNnWe+tsaSsstqYNN7Izwvr/izA23f2QHCAj137cwc1lYUDfLzxfw/EqNwaz1ZSVomQQPPvWaU4OrOtoWnQPSdKen7JTjwwN0Ox9S62n7ggO9HvzzzXJ83WfAsuulLp1BkIf/lkA6b9etDi/T9knTV7+8HcEgDA99fu1ylcaDu/pEyRxOrc4jLkFqub2Jm64oBdvRobj14NGpMXbcOKPbn4aNXV10nJHMnaY/1/W7oLy3eewSwzU/It+VnFOkbmmFtXatuJC0g304MlZWq6ltg77ClFxtHz9Z4PFXNxyYkYnCjkh6yz2HriAgZOW4PlOx1PfPx2u3OTJ835ebf5C3yNrSqVaZaS8JYtoU6L0r7Zmo34D9IwbaXloEmOj1aazjZQbJiszoe3pVySzzccx+Nfbpe0S2tNS7tWWt3ZwXJOkfRgTmqX+pH8S3jnp33IlxkQVBsENh87j9JyaUNck5fVzw+7f24GHlm4TdaxtXhhnpvunBktG48UYOy8zUj4e5pT9q8VGUfP44QHLlYpF4MTJ5j1+2G1myBb6q8H8Ox/rM/O+DLDOeP91hJjhRCY+8fR+rcrUPBNqpKySgghTGZDXSqvMl5gPv1DmQ9jud2+V+xY7VcIgTQbxfTkqvuc2rvStRb85ZP1WLjxhOyZSgs3HseYzzbj4flbjLddqai2qxfhnAYr8B7OK8H9czdho5WhVSUIISxP7T/qeetP1XUgpxhj523GMDco0OhsDE7clNI1WvadLXZ4H3PS6wcRjlqxJ9ds4nJdP+yyv4iXLb3e/g0PztuC/h+sdtoxzFl3+Bxmrz1i8bX+QmKweCT/EhZsOI6KKoPVvB0CyiqvBoh7z8j7e1h6rVxA7fdqwt9XI/6DNFk9PK626UgB7p+7CYfzSqxu9+TiTGw7cREPfb7F6naOeuuHvRj80Vp8sekECi9XoKTM/BIWRZcrkbriAF77n7LFK7/echJDPlorq+dCyU9iJT6HPQUTYt1UYqr9xb3UYK7DQkonxlELxedK6pT4P3XBuReAjGOu/9ZWeLkS01cdwvRVh7DkiQEY0L65XfvZfboIu08Xoayqut7QETlPzYrTW49fwKjYViq3xrwHrwUbT3yVibUvDbO4XYGN3jClhpcWb746C+79X/Zj6o9Xax4dT72tXu9qzLu/Gf+vZKLpG8uvrms25cd9+PLR+Hr3K52zRpax58QOw2f9gX+tsT10k19chm+cMOUUuJpAqbaLpRXYe7ZI0rYCqDclUuDqLBx7Kjs6a4hJq8Z8ttnhfWRJ6IFyJ3IvFK4s1iWXq6pVW+KsZQ6k+jLjBH7adT3nrfZSGbZm+V6uuP66Zp+/jF9258h+Puu+l7RQ7+pIfgk+STssOY/J0zA4scOfeZcw47frMwXM/SEs2ZqN+L+nmU188xQjPl6HvGJpH2pCAH3f+91kLaNqg0Dyom3Oap5ZrkggPF5gXyKoBsvEKMZgEHhgboaqbVhbJ9dm/objGDV7I4quuHb167oXvtQVB9Bu8grj71N+2Ifdpwtl7TPz5AWM+tcG7MhWZrVrIQROni91SdB0oqAUU37Yh79+43hF4iHT1+KZ/+zQ3OwseyTNXIf/+/1Pq7MU5ahb3PN/mac1XTeIwYkCzK1o/MGKAyq0xLXsSXx86btdJr8XXXbNhaGsshof/LIfW1ww4+ipxZZXuFaTkoWmavdC6KDD5+utJwXvO1uMrSeUf+5/25creV2eumf/3s/7setUIeatc+16KXUTkj+tc/zMkxdx5782ytrnvXMysOt0Ee5TqIjf5+uPY+j0dLz94z6UllfZDJxr9zzITVa/eNmxInPmvnBsd8J7TS07T9kfcO4+XYg9p4vw654cLNp43OS+F7/bhTe/3+vUqd+OYM6JA6qqDZi/4ThW7HHOmjwNQYUD3adnCq/g1o/X4ZEb2tncdt/ZYo9INpvhwOJmSs7S+b5Ouf/3f7EejNs7u8qW2kXk7HXFxcM938pca0sOpeLPD69Nj/8i46TkxGvSlisV1ZKC3Mvl1UATFzRIJvacOGDx5pNIVajLjeS7YdoaFJdV4Z9p7jd12x6nLlzGvywsC19SVolsC1MwnUHuRdAZCytqscYHSbPmYB7Gzd8iaSZT+qF82TOelu08g798st7pK1bP/eMo5m84bntDJyspq8Sve3JMejQvuXmuCoMTB+yxMN1QQDCnu4E5r0BCoa3x/RfMXOCTZv6BlG+z0O/91Rar5DqblAvA0XOOFZVyZu6DuV0r+cGudNlypzwXdXbpzNV3NxwuwKOLtmP94QK8eW12jDWPfbFd9uzEkrIq7D1TjHd+dN4q5+dKyq3mg1QbBDYddc7U/fziMsz87ZCxIvkTX2Zi0tc7MFWFVd2dhcGJA/63w3wV11MXrhinEZJjftmdgyqD+pnztlRUG3A4r8SpF1FzKxYfyb+EZTvOoFzFdTvM5bJYKqRlr3clrOgsVVW1AesPn7O5nZRtpHj5v7sVC1DmrTtmkjxrbsbSi9/uqndbaXkVkmb+gQ9+sf08OnuhwdqF6pSYJWStB03OkN1Fmef9x5/W3x8LNhzHg/Oun2tOUZliQe/EL7fjn2uOYNy157Km1MF/a12TDudbr12jdQxOSNMO5ZVg4xHtV4Z8/pss3DJrHT50oI6ItUq57mbwR2sV3d93mfW/CFh6ukrLq1BtEPjFwoyNQR+uxbj5W20ec4mN3JC1B/MlrwR+QqEVw+sm2m84UlCvFpC5L03/zTyNI/mXMG+99SGIs4VX0Pe9361uU2gmgdUVb11nHmPsvM3oY+O85ar7Wn248iB6Tl2lyDDQrtNXSzhY65F8ToHZT2picEKkgJqZKOZK7Uvl7G+sDUF+cRl6TF2Fe+ZsqjeL7lBuCeZvOF6vRtClcvkzxjYdLUDyom0YOj1d0vZbjl+A4VoPk9J9aw/Oq18Dp26OhtRhmrVmFh6sy1YvlrM6D2vXPlHa/hzTIfragVDt3lAlAqT3ft4vOah1hLv33jM4ISKPsXLf1Zlzu+rUdACAf6Qdxns/17+wfrv9tOxhqLo1I2x56/u9WLjphKzH1LXPQsFDc7WGpCz5UFtJeZXkb9rbTl7At9tOmQyD1A5I/rRRCt9elgJ/Z1RtvVKrsNuRfOl1i05flPY+Kr4iPXBoqFVpGZwQUYP3466z9XJMVu1VtkTAd9tPoaLKgGI7i7498aX0adM5RWX4cddZWZVOf9x1FqkrbM8+PHXhCl7532489sX1AooLatXQ2GRlVV2nVF61ce2eseoQnpC42naNHbWCOzm5wYV21m26UFqBd3/aj4M58ssd1C4iqGQtI7WxzgkRacI3W7MR17Yp0g7ko0+bEAQH+KBbyyCL2yv9jbJuHkqVQeDYuUto36KxrP0IISzmD904I93u6a1ykilreojO39EdyRLqANlzjB1WemeyThUiOrSR5H05y/rDBVh/2P4ZM3IT3DOOnsfmY+dxf1yUrMe9+f0eu+tlvfhtll2Pq+FIrSlnYnBCRJpgbqkHc4u+udLZwjJJwUntJk5YuM3sonGAtGnXStpwuADJN7RTfe0ed/TG8j1Yd/gcPh7dR/JjapJg5VaidqRA5OoDjhVXHD5rHe6Pa43p98c4tB+lcViHiNyTRofi19mYYirXWZkFyGoTAHZkX6y3irdWaDlk+npLNk5duIJltWY/SY3x0m0kFt/17434bvspSdOX95yRtriqI8zNhlMbe06IqMH745C0gKKq2oDP1h/D7lPXLxjmkm+VdMyBAnZrDuZjTZ1lC/KLyxAW5O9osxTjaM+Ys2PU2s2b+8dRtG4agNH95Q3b1FVtEHj5v7vh46XD4Q9us7n98YJStAtthDe/34Nzdqxp5o4YnBCRphksJPkpeVGytChh5smLGNQp1Pj711uy8VGdWja/7Mmp943a1SsdyzHms82Y83AcOrRwPCfk5PlSs8MK01cdwp0xkdDrr79Kpy9exp7TyvcCrNrn2rXN3vx+L0rLq9C8sZ/FbaROe5a63b6zRaioMmDx5mxJ23sCBidEpGnfbJP+gfyjwiX8Z63+E1tPnMf5SxVY/HgCDlmYJnu4znTTmHd+U7QdSjpWUIoRH6/DqNhIh/dlqc7LmcIr+Gn3WYyKbWW8bdCHyhbmq+HM+icAkFtUv6di24mLihWFm7BgK06etz0FecTH6yTtL/v8Zfxx+Jziyya4GoMTItKsc5fK8YaE9VdqbDdT4t9RNRWK/7Ha/AKTZS5a1Vjp3hhnr8V0trDM9kZuYPWBPLO3/77f/O1y2SqDL9eQ6c4JAl2NCbFEpFnxH6RZvM/VyZSWvoket1DTQ0ly130hcnfsOSEit/Sbi3MN0g7moW3z+nkajtTRkOp8qecmQbrjNGdLvSmkHAYnROSWnJ1rUFfBpQoUXGIPhpJ2ny7C3f/epHYzSIM4rENERIpzVS4OeSYGJ0RELuCGoxcOqTK492wRUheDEyIiF7A0DZnIlkqNrn/jTAxOiIg0zl2vTb/vz8Pyndorje5u/rZ0l9pNcDkmxBIRadzzS3aq3QTZjp0rxey1RwEAR+oUqSPtqao2wNtLO/0V2mkJERGZdTDX/YaEft17fap3TZBC2vX80iy1m2CCwQkREVED98vuHLWbYILBCREREWkKgxMiIiLSFAYnREREpCkMToiIiEhTGJwQERGRpjA4ISIiIk1hcEJERESawuCEiIiINIXBCREREWkKgxMiIiLSFAYnREREpCkMToiIiEhTGJwQERGRpjA4ISIiIk1hcEJERESawuCEiIiIUFFlULsJRgxOiIiICAYh1G6CEYMTIiIi0hS7gpPZs2cjOjoa/v7+SEhIwNatWyU9bsmSJdDpdLjrrrvsOSwRERE1ALKDk6VLlyIlJQVTp07Fjh07EBMTgxEjRiA/P9/q406cOIGXXnoJgwcPtruxRERE5PlkByczZ87ExIkTkZycjO7du2Pu3LkIDAzEggULLD6muroaDz30EN555x20b9/eoQYTERGRZ5MVnFRUVCAzMxNJSUnXd6DXIykpCRkZGRYf9+677yIsLAyPPfaYpOOUl5ejuLjY5IeIiIicp6SsSu0mGMkKTgoKClBdXY3w8HCT28PDw5Gbm2v2MRs2bMD8+fMxb948ycdJTU1FcHCw8ScqKkpOM4mIiEimi5cr1G6CkVNn65SUlGDcuHGYN28eQkNDJT9u8uTJKCoqMv6cOnXKia0kIiIiDc0khrecjUNDQ+Hl5YW8vDyT2/Py8hAREVFv+6NHj+LEiRO44447jLcZDFeLvHh7e+PQoUPo0KFDvcf5+fnBz89PTtOIiIjIQ8jqOfH19UVcXBzS0tKMtxkMBqSlpSExMbHe9l27dsWePXuQlZVl/Lnzzjtx4403Iisri8M1REREGiGgna4TWT0nAJCSkoIJEyagX79+iI+Px8cff4zS0lIkJycDAMaPH49WrVohNTUV/v7+6Nmzp8njQ0JCAKDe7URERESAHcHJ6NGjce7cOUyZMgW5ubmIjY3FypUrjUmy2dnZ0OtZeJaIiMidaCnnRCeElppjXnFxMYKDg1FUVISgoCDF9hv92i+K7YuIiMid/fr8YHRrqdw1FrD/+s0uDiIiItJUzwmDEyIiItIUBidERESE7AulajfBiMEJERERYf3hArWbYMTghIiIiDRU5YTBCREREQHQqd2AWhicEBERkaYwOCEiIiIO6xAREZG2sM4JERERkQUMToiIiEhTGJwQERGRpjA4ISIiImgpJZbBCRERETEhloiIiMgSBidERETEnhMiIiIiSxicEBEREQQTYomIiEhLOKxDREREmqKh2ITBCREREbHnhIiIiDSGOSdEREREFjA4ISIiIk0lnTA4ISIiIk1hcEJERESawuCEiIiItDSqw+CEiIiIAKGhucQMToiIiEhTGJwQERERh3WIiIiILGFwQkRERCxfT0RERGQJgxMiIiJizgkRERFpC6cSExERkaZoJzRhcEJEREQaw+CEiIiINNV1wuCEiIiINIXBCREREUFoqOuEwQkRERGxCBsRERFpy4XSCrWbYMTghIiIiFBl0E7XCYMTIiIi0hQGJ0RERMQKsURERESWMDghIiIi6HQ6tZtgxOCEiIiINIXBCRERETHnhIiIiLRFO6EJgxMiIiLSGAYnREREpCkMToiIiEhTGJwQERERF/4jIiIisoTBCREREWkKgxMiIiLiVGIiIiIiSxicEBEREbSzsg6DEyIiIgKHdYiIiEhjuLYOERERkQUMToiIiEhTGJwQERER2oU2UrsJRgxOiIiICCEBPmo3wYjBCREREUGn085kYgYnREREpCkMToiIiIhTiYmIiEhbtBOaMDghIiIieED5+tmzZyM6Ohr+/v5ISEjA1q1bLW67bNky9OvXDyEhIWjUqBFiY2Px1Vdf2d1gIiIi8myyg5OlS5ciJSUFU6dOxY4dOxATE4MRI0YgPz/f7PbNmjXDG2+8gYyMDOzevRvJyclITk7GqlWrHG48ERERKcOth3VmzpyJiRMnIjk5Gd27d8fcuXMRGBiIBQsWmN1+2LBhuPvuu9GtWzd06NABzz//PHr37o0NGzY43HgiIiLyPLKCk4qKCmRmZiIpKen6DvR6JCUlISMjw+bjhRBIS0vDoUOHMGTIEIvblZeXo7i42OSHiIiIGgZZwUlBQQGqq6sRHh5ucnt4eDhyc3MtPq6oqAiNGzeGr68vbr/9dnzyySe45ZZbLG6fmpqK4OBg409UVJScZhIREZFMGppJ7JrZOk2aNEFWVha2bduGDz74ACkpKUhPT7e4/eTJk1FUVGT8OXXqlCuaSURERBrgLWfj0NBQeHl5IS8vz+T2vLw8REREWHycXq9Hx44dAQCxsbE4cOAAUlNTMWzYMLPb+/n5wc/PT07TiIiIyEPI6jnx9fVFXFwc0tLSjLcZDAakpaUhMTFR8n4MBgPKy8vlHJqIiIgaCFk9JwCQkpKCCRMmoF+/foiPj8fHH3+M0tJSJCcnAwDGjx+PVq1aITU1FcDV/JF+/fqhQ4cOKC8vx4oVK/DVV19hzpw5yp4JERER2U1oaDKx7OBk9OjROHfuHKZMmYLc3FzExsZi5cqVxiTZ7Oxs6PXXO2RKS0vx9NNP4/Tp0wgICEDXrl2xePFijB49WrmzICIiIodoKSFWJ7S00o8FxcXFCA4ORlFREYKCghTbb/Rrvyi2LyIiInf2UEIbfHB3L0X3ae/1m2vrEBERkaYwOCEiIiINZZwwOCEiIiKNYXBCREREmsLghIiIiDSFwQkRERFpCoMTIiIi0lSdEwYnREREpCkMToiIiAhamkzM4ISIiIg0hcEJERERAdCp3QAjBidERESkKQxOiIiICMw5ISIiIk3hVGIiIiIiCxicEBEREXTayYdlcEJERETawuCEiIiImHNCREREZAmDEyIiImLPCREREZElDE6IiIhIUxicEBERkaYwOCEiIiIIlq8nIiIiMo/BCREREWkKgxMiIiLSFAYnRERExDonRERERJYwOCEiIiJNYXBCREREmsLghIiIiDRU5YTBCREREWkMgxMiIiLibB0iIiLSFpavJyIiIrKAwQkRERFpKiOWwQkRERFpCoMTIiIi0lLHScMOTuY+HIeRPSLUbgYRERHV0qCDk5E9IzB3XJzazSAiIlKd0NBc4gYdnBAREZH2MDghIiIi5pwQERERWcLghIiIiDSFwQkRERFpCoMTIiIi0hQGJ0RERMRVid3FtHt6wVuvw1ePxSP1nl5qN4eIiKhB8Fa7AVo2Jr4N7otrDW8vPaoNApOX7VG7SURERE6hoY4T9pzY4u119Sny0uucdoyWwf7o1jLIafsnIiJyJwxOAAT5X+9ASmzf3OJ2Tw5pDwB4KKGNosf39dYjwIcvBREREcDgpJ7kG6It3vfqyK74+a+D8O6onvXuiwjyN/5/+5tJso+r0zmvZ4aIiMidMDgBMKhTKAAgrImf1e30eh16tgq2OcTj7cQhICIiImfQ0sJ/TIgFkHp3b/RqFYI7Ylpi/9litZsDAGgVEoAzhVcAADFRIdh1qlDdBhERkUfTTmjCnhMAQHCgDyYN64DWTQPVbgoAoImfNzjKQ0REDRWDE4V0jmgCAPBnYisREbkjDXWdcFhHIY39vLD77eHw9dKjrLJa9uN1Fn+p9ysREZHihIaiE37NV1CQvw/8fbzseiyHcYiIiK5icGKnz8bF4e07uht/D/CR3gkV3VwbuS1ERERaxGGdOpo39pW03fAeEQCAQF9vfL3lJF69tYszm2XVnreHo9fbv6l2fCIicn9eeu30VzA4qaNvm6b4W1JntGvRSNL2D/SPwgP9o0xuk1tQLbF9cxw9d8n4++BOodh9ukjy45v4+0jabnCnUKw/XCCrbURE1DAMvlbzSwu0EyZphE6nw/NJnXBnTKTd+wgOkBYs1HjzL92hq5X2mnpPb7uPTUREZA8tFRBlcOIk9/RpJXnbxn6mHVh1gxslkmX/eHmYw/t4cmh7xxtCRERkA4MTFTw2qJ1Lj6fTAW2bSxumIiIiUhuDExU8PKAtBnawvPqxVulYcYWIiFyAwYkKdDodukYEmdz2t1s6AwDGxrdx2nE7hTVx2r7l+PmvgzC6X5TtDVUQYGedGqW0l5iITUTkyRicaERih+bY/fZw/P3unk47xoMJzgt85AgO8EFkSIDazTDr3jjpuUJyPHdTR0nbLX4swSnHJyJyJwxONCTI38fmNOQu4fb3fvh4cVjGFmcNXflJ7JFhpWAiIgYnbuEvva9Oa45uHih7mjLAtXmc4Z9j+zhlv+6W17PgkX5Oey6IqOGyKziZPXs2oqOj4e/vj4SEBGzdutXitvPmzcPgwYPRtGlTNG3aFElJSVa3p6um3xcDAHjjtm54ZGA0FiX3x/fP3GBx+094gVCEpZ6L755KNP6/Y1hjRDWVNyzVWub27iK6eSO7AmYiImtkBydLly5FSkoKpk6dih07diAmJgYjRoxAfn6+2e3T09MxduxYrF27FhkZGYiKisLw4cNx5swZhxvvyRI7NMfhD27FxCHt4aXXYViXMIQE+qJDmPmEyS4R14d74ts1M7uNK76Vvzuqh9OP4Wr9o5uif7Tpcyq3CnBYE38lmyRLEz/nFYLWzhqmzqfVJG4iTyQ7OJk5cyYmTpyI5ORkdO/eHXPnzkVgYCAWLFhgdvuvv/4aTz/9NGJjY9G1a1d8/vnnMBgMSEtLc7jxns7Hq/7L89qt3TA+sa3Vxy19YoAix1/29EA8MjBa1mN8zbSZgG4t1ZspFdrEz6n7F8L5IcqNXVo4/Ri2xLVtqnYTiBoMWVeSiooKZGZmIikp6foO9HokJSUhIyND0j4uX76MyspKNGtm/ts9AJSXl6O4uNjkx93c3dc5sz6CA3zw7ijrM3osfauXm2zZt01TdApvbPx9wsC2mipvrCW+3tb/lFxw/bborb90U+/gCvH38cL9ca3VbgYRuYis4KSgoADV1dUIDw83uT08PBy5ubmS9vHqq68iMjLSJMCpKzU1FcHBwcafqCj3604d3KkF0l4c6rT9h137NhwSKH28v1VIAGKjQuw+ZsvgABx4b6Tdj3eFlsHqDJ+ENrK+mrXU2MQZs3XUHFJSktozmUSDGsQiUpdL++CnTZuGJUuWYPny5fD3t/yBOXnyZBQVFRl/Tp065cJWKqdDi8YW73P0g+4/EwfgjphIfPtkou2Nr9HrdVj+9ECHjmtuqElLerUKVrsJmvKeC3KA5Obf2HcMpx+CVPLczZ3UbgJdI3WFe1eQdaUJDQ2Fl5cX8vLyTG7Py8tDRESE1cfOmDED06ZNw2+//Ybeva2vuuvn54egoCCTHzLVMawxPhnbB50l1D0Z3On6eH3dC8m9fdlVXpvUa2CQv7wk07p5Gbf3binr8fYK8HVeMqzSamajyX1u+zEXxK352RgSJde5uWuY2k0wkvWu8PX1RVxcnEkya01ya2Ki5W/wH330Ed577z2sXLkS/fr1s7+1ZJePR8davM+dvpH+YGUqtVI6hFnu7aqtvZVeMSn+7/4Yhx4vh7V8l9DG1oejHDHtnl6yto+NCsGJabdjjBOXcFBDUxlDr0Rq0msop1B2yJqSkoJ58+bhiy++wIEDBzBp0iSUlpYiOTkZADB+/HhMnjzZuP2HH36It956CwsWLEB0dDRyc3ORm5uLS5cuKXcWZFVTG/kQ7iJGQr6MpWnUUnw8OrbelGFXayKz18ARncMbI8JJOTqrU4bYHWSYq5uiu/bPHfX0sKFGVwwVEskOTkaPHo0ZM2ZgypQpiI2NRVZWFlauXGlMks3OzkZOTo5x+zlz5qCiogL33XcfWrZsafyZMWOGcmdBBGDG/TGYMDAa0+7phT9eHibrsZ3DG+OuPtJmWJmbOmsrg0hqhlGgGw3DWFPzFNlToO3RG9rhlu6mSfdtmgfK3o+tKfeu4oqcHFdytNdQCRr6go8B7dX9QuOp7Brse/bZZ3Hy5EmUl5djy5YtSEi4vlhZeno6Fi1aZPz9xIkTEELU+3n77bcdbTu5MWd8Xt8X1xo+XnqMiW+Dts3dZ3XfhGu9PXfGRDq0n56t5OVmuaIn4ve/DZH9mABfL8wbbzr8a89YuK0p96QN9tTJWf6084d4tap3a8/qibOEmUikWQE+XsaLUlK3cKvbdo1Qr8iZPWoHZ+/f1RNfPhqPj+6znihuD1sf/K0srA79xJD2+Ppxx1dIDgtSZtion5XhtpThnRU5BkmjZEj7v0nSZxvWJmWI11VcXcOosRMrPmsJgxMXqbkYtbhWn0Qr4+c6XJ35Y4k9f3ipMhMhzZlxfww2vnYTPh4Ti49Hx2LWaOsJpHfGOtbr4K133p+Cn7fe6vPo663HkM4t4C9x5WIlvXeX+d4FvU4nqSZOkL+PSyrEWrLn7eEY2CFU0X1+Pl7ZpH1t/KVrU1xbaUMilt6njpr7cJxT9kuOY3DiInfGROKbiQPw2wvyu7mdLdC3/kXxsUHt7N7f2Pg2+PP9Wx1pEvpHN0WzRr5o4u+Du/q0Ms6/l1tOX6qZNoIfR2yefLNLCnh1CZc/5T6siT/eudN8gmMjP+96uR91tXByaXxbnFGXIcnGOavN2uw7TzXKzJcPHy95Yd+n4+oHIiN7Wi+BQephcOIiOlxdzE+LM2fMffG9w8H8B1vl3O31+GD7gyZL5j7cF10jnFdLx9mv+c9/HYRHb2iHKX/prvi+7d1nsJXps0M7q79Ojj3sXX359duUXT5ATlVoTyan93lCYluM6OG5gYi/j+ddyj3vjDTK2Rn7Si249/CANkh/aZjVLv2ahKznHajsqOZaM3Ip0lYr+3B0/z1bBWPKHd2tBgT2at3UfE6KJYsfS8Cn4+Islsxf/8qN+OLReCWa5nK3dLfv4tahhfrJ2REK5f4AUHycSsnPAq0uPKpkrRtzl5LdU0cotn+t0OYrSbJFhzbCgwlt8PSwDg7t5/27eiE61PqH6fKnb8C+d0bIvnC1bhqA5o18ra5/0yMyCNHNA9G6qfypo+7E3pyj9tdeG1sJwkrR6XQY3El6TsegTqFWv6FGNXPt67r+lRsV25cWpq/OsLN4X4CZoVu66s3bnb8w5mu3dsVBhdYlG92/DdrU+TuS21PtDrk2DE48yN/v7oVXRnZ1+nG89Do0kpEx3jWiCVa9MATpLw3D5tdvhreVbzc/PTsIaS8Og5dLrwTyjtVYpfUn9Dpg5QtDsPX1m01Wi7bFnhohtd3jpBW2XUFuMPSPMbGKt0HJXtP7NLAys1aS+e1xj5laRo8Pbu+SY9dOeB/Wxf6hzSb+3rLrOLkjBicN3CAZ34rtdXefVugS0QTeXnqbCwfq9Tq7AhMpXcMv3uLYlNO5D/dFz1ZBmPmA9G+vNTM/HO25fvGWzlidMhS+3nqEBflLvjxEhgQgOMAHG1+7ye5j3xXbCt8/cwO6tVR3jSt7r/FNZATSnpogqeaMKiUo1fwBHZorsyMHOTod2FzA+9xNHdGnTYhD+9USBice7okh1r8VOFr4y53c3rul1WnTtozs2RI//3Ww1dWm6+oWafuCLuVz9683d5JVmVOvA964rRsGXvswtlTPRArdtWnFQ1wQyLqzD+7uia2v3yxrpXB7WCowOKKHtmcZaYJ7x2hWpQzvgpkPxKrdDMUwOPFw5mZG1HyIfTNxAHQ6nUumuWrFKyO6KLavn/86yO7HOrui+ZDOLTBxSHurQwpy2/BCUme8dqsyw4Yf3O151VsfSmiLsCB/xLdrht9qVcZV6qVe8sQAAEC70EZY+Eh/jBtgWp7fzTtHyApbU/o9EYOTBmjOQ3HIfDMJiRrp4nQVnU6naKEzd1vQ7X+TBmLhI/0lbWsucAnw9cJTQx1LuK4R6UBPjlp0Op0xQOgY1hg/Pmu5hLozUqYGtL/+93pj1zD00uj7r3YCtSOrXqe/NEyB1ri/sfFR9ZZzaAgYnHg4c9+m9HodmjdWt3iWpxhyrWdqnI1F5prXqnWi1qyPuLZNcaMda9RYUlNCvCF9YR/QvjmOp96G314YAr2VriepvRj/d38MnhzqmoRMZ3rxls5I6haOvXVm8W17I8nufTp7hpEjQ7yOuqGjc4ZIpX60uMNalAxOiBzw2bg4/G/SQDw1xHyPQs1nQO0ZSq6YiSSniz/MziqvcqeSu7NptZZk0Ol00Mt4Da1dCO6Na43Jt9o3lVVqMvt/Jjq+RlJt5s7nrzd3wucT+l1L9NTV2lYbV0Fzazx9Vys3qH2t8gltzcxua+TrhdDGvuhRK4cs5ZbO8Nbr8FatQoV1p/jWtem1mzD7wb54oF+UrPY3RAxOyKrIEAWLNynA3iqd1siZlluXv48X4to2lXWx0lpuwIgeEXhqaAez5b3pKqWG8CbY6GGTIzIkwKRnwtLbqomfcn8zT9pIsLeHon8OZv4M7+3b+vpyHLXut1S5+Y+X69fGualbOLa9kYRBtXo8nru5Ew6+N9LkvWHri0dkSABu793S4narU4Y6XKvKHoM6hmL+BG0NHTWM5Q3JKmsXyxu7hOHlEV1MvjHUCPSV9vax1v1dm5+EQkJN/H0Q5O+N4rIqSfu0Ju3FoThXUi5r9o0n0ut1iiW6eqqwIHm9S5b+pNo0b4RZo2Pwt6W7JO2nTbNAZF+4bPF+V65tlDH5JrQMDsDmY+dNbl/z4lDnH1xCBPNQQht8l3na5LakbmH4PxlT/60x1wtkrWaTPTqGNUZih+b4d/pR422u+DKzWIEVyJXGnhOySqfT4ZkbO2JYl/q5CiN6hOOW7uF4ZaT5GTBPDGmPdqGNMCbeehfm4scS0CmsMf4zcYCkNj2pUFJmhxaNTZIMicx5ckh7i+X4pah7UZNTxExOsUNn6te2KVoGmx/GkzPFXQ45A0JLnxiAty0sYEnuicEJ2c3bS4954/vh6WEdzd7/+m3dsPalYTZXjh3UKRS/pwxFXNumko47cXB7vDuqB9a8OBSR10rhm5tqN298P0XXtFBK3S9Cblsgy8XNdkX6go/e9CPxs3FxmKzgwn0D2jcze7ulQnEf3dsbjf28MfUO5Rd1rK12jZRZZlbort37qY0sElMJ7ZvbLPDoLtz140BpnvFqUoPi663H+MRotG/RGGteGoZNr92EzuFN6m13S/dwbHnd/tkCcv1vkrTiW0IITX7AN3SPDIxWLHeoaaBpPsOOt27BiucGo0ek+dyV5c8MNHt7r9bB2D11OJJvUH41bkuF+Ub0iLA6VKN2kuv7d2m7Rs7ixxwbImFschWDE5U8Prgd/Lz1eDChjdpNUYVSn2/+Pl6aqZkR17YZJrkomc1Wb5QreFrxvtqzLhzVookfPhsXh6+vjeU3a+SL7laqBXcMa2JxMUc5AZNSnDVUowRrK6ZrwaBOofXWvlF76Qd3xOBEJZEhAdj3zgj8/e5etjcmRXhSb8UjA6MxtHMLi5VWtRQ2JN8QDeDqGktqeF3iqrNKT/Ee3iNCVj2LQR21m//k6kA0RGPDsTVDr/Y8C3MfjkPv1iFWtwn09cKH95q/FtT00N0RE4lOYY1xQwf7a6S8d1dPrE4Z4hZDR9rItmqglM70Js9wR0wkftp1Fv+ZmIAvNp0wm9MT4OuFLx6NV6F19XnbuKi/fls33NqzJWKi1KloOja+DW7qGoaEv6cput/aF2xztTEsqSnc16XOUOTDA9ri7Z/2298eN7jg2BLTOhjT749BZEgALpdfn5FnLjiKahaAo+dKXdk8u9zY1foKxF0jmmDlC0OsbgMAn4ztc3VIWEa3c3CAD4quVAIA/v1QX9zWqyUA4Ei+9p83BidEFjx7Y0f8a+0Ryd+8zQm0o8rlP8fEYsb9veHn7YWBDnxLcoS5qeOWJLZvjv7RTdElon7eDwD4eOkR3858IqirhJspwqWktBTp02mbNfLF3ndGIKDOUgreXnpENw/EifOWpw57kn+Mia1325ePJiD4Wq/J5Qrz5QKaXatP8vmE/khdcQBP32g+Id8R8dHNsPXEBcX36yhbgUmbZoHo3jII+3OKAVztoZ/5QAwCfNX7LLEXgxMXcdsZGQ1M7b/9l0Z0wXM3d4KvhPor5sx8IAYhgfULPdWbrVOvDTr4eTu3dLclq14Ygl2nC3H7tW9YUnh76fHdU+YTOi2Rsi7MW3/pjvd+ltaT0KtVMPacKZLVBimk/tXK7QVtrJEpws4g9Yt9o2t1kmp/NAZbGc5ZmNwfxVcqjTlm7UIb4TMba878pbe0VdfbNg/EsYLrvQlLnhiASxVV6P32b5IeryRHrhV6vQ4//3UQ2r++AgDQKsQfN1vIZdI6jisQWWFvYAIA9/RtbfE+tWc8WNIlogke6Bfl9PaFNvZDxuSbrM68iI+W3ttyZ4y0i5Cna26h6qmS1Hrr3tglDKNipect/fepRIyKlfa++PDe3hgVG4n/PnV1xp1er0OQBpLO7aHX6/DNxAG4vVdLt85p9NzwXWO0ejFqaORW+iTnaRkcgLHxbXDyfCni2tYPRLT+J6O9zlCBLx+Lx9s/7sMrI7vi/rkZ0h9p57l0tTCUp5bab5l+ZoJbSxV1w4L88Y8xfRRrhyPvDSXeVokdmrv9qvMMTghv3N4ND87b4rJpsGrqGhGE9+/qiZbB2lkzSHsXOWmUaLeXXoc3bnd8Cq/WAxmp/npTJ7z4nbTS9ub0iAyWPcTmiO+fucFlx1LCsM4t8PSwDhbrzdTVObwx/sy7hLtk9NjUJacicF1B/t549iblc2rcAYMTwsAOoTjw7kinL1GuFQ8PUG7xNXs561p6Z0wkftx1FpMUKvGvLR4SgVhxb1xr/Lo3F6sP5Cm2T0uBW0SwP84UXnFo3/4+7vWZodPp8MpI6etI/fDMIGRfuGwx2bve/u1tmAVZU4arUudGC5hz4uGk1ifw1MCkdu2KICesaCxHTe2GGBs1DxzxjzGx2PnWLW7fpesOnNXh1bqpfUUFpfZkfTNxAG7qGoaPR8fadRy5YuwsmlZ7BXKpi4wqLcDXyyQwccnEhlqHaKiBCcCeE/JwXnodvng0HhVVBuMURLUsf/oGfJVxEk84Ydn5GjqdzuJS8HW5ckVbJXjK0I3azOUjhAf5I9DXC37eevjbmCkm92X4+vEE7Dh5EeMXbJX1OD9vL2x5/Wbo4FhiuhrcdKRWUxickCpcmWcxtLP1Ikiu0i60EaZcW8DNYFDv42tRcn8s3HgCqfe4bya/q0Q3b+TyY6oRhHnpddg55RbodTqb39bltq+xnzeGdG6BGzo2x8Yj52U91tn1aVzB0vM1pn8Ulmw7hb/d0tm1DXIT7hWOEpHDhnUJwxePxmtmTSJL2siouirFe9emLb9zZw+b2/707CB8Oi5Ocq6BJ/Dz9nLpyr7mFut0dzVfOaQM/6Te0wvb30zCiB4Rzm2Um2LPCZGCGnlo7o4agvx9kDH5JvgqdMEcN6At7oqNRBN/H0z9cZ/VbXu1Dkav1tZndEQrHDxp2dsSAroaA9o3x3+2ZNvcrib4bOzv2ZchSx1NOp0OoY3da2jVlTz7XUHkYo/c0A7rDxfg1p78NqSElsHK9u7UXc3ZS69DtZ1DbCGBvvjj5WH1ytB7IqlTbwHgjt4t4eulk/yYhxLaIONoAW7sGmZv8zyKpwdrUvFZIFJQYz9vLH0yUe1mkEThTfyQMryL3T1ebZ2QkxJs56wyrSRh6nQ6jOxZf/mDgR1Czeac+Pt44fMJ/V3RNLfQr21TPDIwGh1auD7fSUsYnBDV0kKVblatXFYapvviLC8zoIaJg9sj61QhbjNzgXdnTwxpj2U7TrvFSsJq0ul0sobRPBUTYolqad7YD0ueGIAfn3Vu5UtPmBarpanIcpeHGNM/CgDwggZnSjTy88ai5Hg8cK2N2mPfm9fHS48H+jn3nJz9dyV1liG/bjiOPSekKHPrWZij5YvzgPYsYGbN5+P7YdnO03jxli4uPa6Sb5nUe3rhuZs7aX7GErk/rqtmHwYnpIjMN5NwvrQCHcMaq90UcrKk7uFI6u6ey7DX0Ol0DEyINIzBCSmieWM/NOe0OCLNiQ5t2ImV5J4YnBCR27u/X2vMW3cMeSVlbrvKs9J2vz0cFVUGNPbjxzy5HybEEpHbC/L3wabXbkLq3Q2vJH/L4Ksl3kfWqTQa5O8juciXnAXtmEJhGwNkxzGkJlIZP8iU0VBXcP31+cHYf7bYZYncDfNZth+fL/swOPFwfjZWGCX1MTYhR4QE+mJgx1C1m0GkKAYnLtK3bVOXHu9vSZ1xrOAS+ke79rgkDacXOgcDPXImW+8vd/mrdodORgYnTrb2pWHYdvwC7nVxFcrnkzq59HhEROQehnXR/jpGTIh1snahjfBA/yh4uUOoSkTkRM6uKhwS4OvU/XsKX289btF4rSL2nBCRW+BImP0aKTid2JEhyVGxrZB1qhDx7aRVkpZr/iP98Mp/d+OVkV2dsn/ptD/AqPU/JwYnREQe7uPRsXj2PztVH+710uvw7qieTtt/j8hg/PLcYKft3x4Mqu3D4ISIyMN1Cm+CVX8bYvF+7X/PJ6WNTWiD3/bnoW+bELWbYhaDEyIiogbmxi5hSH9pmGbXmGJwQkRuJ6yJHz66r7fazSBya1ped4nBCZHKWCFWvq1vJKndhAbL34eTPG3h37TjGJwQkce4ocPVSqkBPqyM7CxdwptgTP8ohAX5q90Ul7Mn6GDBRfswOCEij9GmeSA2vHojmgay3oWz6HQ6TLuXQ2rkXOyfI1U4q84BUeumgYrW9SDSok/HxSHAxwtzH45TuylOwb9gUsWA9s3xzcQBiA4NVLsp5CZ0mi8bReQ6I3pEYN87Izx2NW4GJ6SaxA6uWeKdiMiVAl3Uc+epgQnA4ISIqMEa2rkF/vjzHMbGt1G7KR6lVUgAXh7RBUH+vMTai88cEVEDtfCR/igpq0JwoI/aTfE4z9zYUe0muDUmxBIRNVB6vc5iYPLwgKu9Kc/c2MGVTXJrLUMa3vRqZ2FwQqSSFk384K3XoW1zJgWT9rxzZ0/8+vxgvHhLF7Wbohk3dQ0DAAT6mtbR+WbiANzSPRzT74tRo1keicM6RCrZ9NpNqDYI+LNgGGmQl16Hbi2D1G6GpgzqFIplTw9EdHPTsu+JHZozwV9hDE6IVOLjpQfjEiL30rdNU7Wb0CBwWIeIiIg0hcEJERERaQqDEyJyC1w/jajhYHBCREREmsLghIiIiDSFwQkRERFpCoMTIiIi0hS7gpPZs2cjOjoa/v7+SEhIwNatWy1uu2/fPtx7772Ijo6GTqfDxx9/bG9biYiIqAGQHZwsXboUKSkpmDp1Knbs2IGYmBiMGDEC+fn5Zre/fPky2rdvj2nTpiEiIsLhBhMREZFnkx2czJw5ExMnTkRycjK6d++OuXPnIjAwEAsWLDC7ff/+/TF9+nSMGTMGfn5+DjeYiIiIPJus4KSiogKZmZlISkq6vgO9HklJScjIyFCsUeXl5SguLjb5ISIiooZBVnBSUFCA6upqhIeHm9weHh6O3NxcxRqVmpqK4OBg409UVJRi+yYi9xTTOkTtJhCRi2hy4b/JkycjJSXF+HtxcTEDFKIG7u4+rVBZbUDftlx4jcjTyQpOQkND4eXlhby8PJPb8/LyFE129fPzY34KEZnQ63UYE99G7WYQkQvIGtbx9fVFXFwc0tLSjLcZDAakpaUhMTFR8cYRERFRwyN7WCclJQUTJkxAv379EB8fj48//hilpaVITk4GAIwfPx6tWrVCamoqgKtJtPv37zf+/8yZM8jKykLjxo3RsWNHBU+FiIiIPIHs4GT06NE4d+4cpkyZgtzcXMTGxmLlypXGJNns7Gzo9dc7ZM6ePYs+ffoYf58xYwZmzJiBoUOHIj093fEzICIiIo+iE0IItRthS3FxMYKDg1FUVISgoCC1m0NEREQS2Hv95to6REREpCkMToiIiEhTGJwQERGRpjA4ISIiIk1hcEJERESawuCEiIiINIXBCREREWkKgxMiIiLSFAYnREREpCmyy9eroaaIbXFxscotISIiIqlqrttyi9G7RXBSUlICAIiKilK5JURERCRXSUkJgoODJW/vFmvrGAwGnD17Fk2aNIFOp1Nsv8XFxYiKisKpU6c8ds0eTz9Hnp/78/Rz5Pm5P08/R2eenxACJSUliIyMNFkU2Ba36DnR6/Vo3bq10/YfFBTkkW+42jz9HHl+7s/Tz5Hn5/48/RyddX5yekxqMCGWiIiINIXBCREREWlKgw5O/Pz8MHXqVPj5+andFKfx9HPk+bk/Tz9Hnp/78/Rz1OL5uUVCLBERETUcDbrnhIiIiLSHwQkRERFpCoMTIiIi0hQGJ0RERKQpDTo4mT17NqKjo+Hv74+EhARs3bpV7SYhNTUV/fv3R5MmTRAWFoa77roLhw4dMtlm2LBh0Ol0Jj9PPfWUyTbZ2dm4/fbbERgYiLCwMLz88suoqqoy2SY9PR19+/aFn58fOnbsiEWLFtVrj9LP0dtvv12v7V27djXeX1ZWhmeeeQbNmzdH48aNce+99yIvL88tzq1GdHR0vXPU6XR45plnALjf67du3TrccccdiIyMhE6nw/fff29yvxACU6ZMQcuWLREQEICkpCQcPnzYZJsLFy7goYceQlBQEEJCQvDYY4/h0qVLJtvs3r0bgwcPhr+/P6KiovDRRx/Va8t3332Hrl27wt/fH7169cKKFStkt0XO+VVWVuLVV19Fr1690KhRI0RGRmL8+PE4e/asyT7MvebTpk3TxPnZOkcAeOSRR+q1f+TIkSbbuOtrCMDs36NOp8P06dON22j5NZRyXdDSZ6eUttgkGqglS5YIX19fsWDBArFv3z4xceJEERISIvLy8lRt14gRI8TChQvF3r17RVZWlrjttttEmzZtxKVLl4zbDB06VEycOFHk5OQYf4qKioz3V1VViZ49e4qkpCSxc+dOsWLFChEaGiomT55s3ObYsWMiMDBQpKSkiP3794tPPvlEeHl5iZUrVxq3ccZzNHXqVNGjRw+Ttp87d854/1NPPSWioqJEWlqa2L59uxgwYIAYOHCgW5xbjfz8fJPz+/333wUAsXbtWiGE+71+K1asEG+88YZYtmyZACCWL19ucv+0adNEcHCw+P7778WuXbvEnXfeKdq1ayeuXLli3GbkyJEiJiZGbN68Waxfv1507NhRjB071nh/UVGRCA8PFw899JDYu3ev+Oabb0RAQID49NNPjdts3LhReHl5iY8++kjs379fvPnmm8LHx0fs2bNHVlvknF9hYaFISkoSS5cuFQcPHhQZGRkiPj5exMXFmeyjbdu24t133zV5TWv/zap5frbOUQghJkyYIEaOHGnS/gsXLphs466voRDC5LxycnLEggULhE6nE0ePHjVuo+XXUMp1QUufnbbaIkWDDU7i4+PFM888Y/y9urpaREZGitTUVBVbVV9+fr4AIP744w/jbUOHDhXPP/+8xcesWLFC6PV6kZuba7xtzpw5IigoSJSXlwshhHjllVdEjx49TB43evRoMWLECOPvzniOpk6dKmJiYszeV1hYKHx8fMR3331nvO3AgQMCgMjIyND8uVny/PPPiw4dOgiDwSCEcO/Xr+4Hv8FgEBEREWL69OnG2woLC4Wfn5/45ptvhBBC7N+/XwAQ27ZtM27z66+/Cp1OJ86cOSOEEOLf//63aNq0qfH8hBDi1VdfFV26dDH+/sADD4jbb7/dpD0JCQniySeflNwWuednztatWwUAcfLkSeNtbdu2FbNmzbL4GK2cnxDmz3HChAli1KhRFh/jaa/hqFGjxE033WRymzu9hnWvC1r67JTSFika5LBORUUFMjMzkZSUZLxNr9cjKSkJGRkZKrasvqKiIgBAs2bNTG7/+uuvERoaip49e2Ly5Mm4fPmy8b6MjAz06tUL4eHhxttGjBiB4uJi7Nu3z7hN7fOv2abm/J35HB0+fBiRkZFo3749HnroIWRnZwMAMjMzUVlZaXLMrl27ok2bNsZjav3c6qqoqMDixYvx6KOPmixa6c6vX23Hjx9Hbm6uyXGCg4ORkJBg8pqFhISgX79+xm2SkpKg1+uxZcsW4zZDhgyBr6+vyfkcOnQIFy9elHTOUtqihKKiIuh0OoSEhJjcPm3aNDRv3hx9+vTB9OnTTbrL3eH80tPTERYWhi5dumDSpEk4f/68Sfs95TXMy8vDL7/8gscee6zefe7yGta9Lmjps1NKW6Rwi4X/lFZQUIDq6mqTFwkAwsPDcfDgQZVaVZ/BYMALL7yAG264AT179jTe/uCDD6Jt27aIjIzE7t278eqrr+LQoUNYtmwZACA3N9fsudXcZ22b4uJiXLlyBRcvXnTKc5SQkIBFixahS5cuyMnJwTvvvIPBgwdj7969yM3Nha+vb70P/fDwcJvt1sK5mfP999+jsLAQjzzyiPE2d3796qppj7nj1G5rWFiYyf3e3t5o1qyZyTbt2rWrt4+a+5o2bWrxnGvvw1ZbHFVWVoZXX30VY8eONVkg7bnnnkPfvn3RrFkzbNq0CZMnT0ZOTg5mzpzpFuc3cuRI3HPPPWjXrh2OHj2K119/HbfeeisyMjLg5eXlUa/hF198gSZNmuCee+4xud1dXkNz1wUtfXZKaYsUDTI4cRfPPPMM9u7diw0bNpjc/sQTTxj/36tXL7Rs2RI333wzjh49ig4dOri6mbLceuutxv/37t0bCQkJaNu2Lb799lsEBASo2DLnmD9/Pm699VZERkYab3Pn168hq6ysxAMPPAAhBObMmWNyX0pKivH/vXv3hq+vL5588kmkpqZqqiS4JWPGjDH+v1evXujduzc6dOiA9PR03HzzzSq2THkLFizAQw89BH9/f5Pb3eU1tHRd8DQNclgnNDQUXl5e9bKH8/LyEBERoVKrTD377LP4+eefsXbtWrRu3drqtgkJCQCAI0eOAAAiIiLMnlvNfda2CQoKQkBAgMueo5CQEHTu3BlHjhxBREQEKioqUFhYaPGY7nRuJ0+exOrVq/H4449b3c6dX7+afVk7TkREBPLz803ur6qqwoULFxR5XWvfb6st9qoJTE6ePInff//d5rLyCQkJqKqqwokTJ6y2vXa71Ty/utq3b4/Q0FCT96S7v4YAsH79ehw6dMjm3ySgzdfQ0nVBS5+dUtoiRYMMTnx9fREXF4e0tDTjbQaDAWlpaUhMTFSxZVenmT377LNYvnw51qxZU68b0ZysrCwAQMuWLQEAiYmJ2LNnj8mHSc0Havfu3Y3b1D7/mm1qzt9Vz9GlS5dw9OhRtGzZEnFxcfDx8TE55qFDh5CdnW08pjud28KFCxEWFobbb7/d6nbu/Pq1a9cOERERJscpLi7Gli1bTF6zwsJCZGZmGrdZs2YNDAaDMTBLTEzEunXrUFlZaXI+Xbp0QdOmTSWds5S22KMmMDl8+DBWr16N5s2b23xMVlYW9Hq9cShEy+dnzunTp3H+/HmT96Q7v4Y15s+fj7i4OMTExNjcVkuvoa3rgpY+O6W0RRLJqbMeZsmSJcLPz08sWrRI7N+/XzzxxBMiJCTEJJNZDZMmTRLBwcEiPT3dZErb5cuXhRBCHDlyRLz77rti+/bt4vjx4+KHH34Q7du3F0OGDDHuo2bK2PDhw0VWVpZYuXKlaNGihdkpYy+//LI4cOCAmD17ttkpY0o/Ry+++KJIT08Xx48fFxs3bhRJSUkiNDRU5OfnCyGuTkFr06aNWLNmjdi+fbtITEwUiYmJbnFutVVXV4s2bdqIV1991eR2d3z9SkpKxM6dO8XOnTsFADFz5kyxc+dO42yVadOmiZCQEPHDDz+I3bt3i1GjRpmdStynTx+xZcsWsWHDBtGpUyeTaaiFhYUiPDxcjBs3Tuzdu1csWbJEBAYG1pum6e3tLWbMmCEOHDggpk6danaapq22yDm/iooKceedd4rWrVuLrKwsk7/JmhkOmzZtErNmzRJZWVni6NGjYvHixaJFixZi/Pjxmjg/W+dYUlIiXnrpJZGRkSGOHz8uVq9eLfr27Ss6deokysrK3P41rFFUVCQCAwPFnDlz6j1e66+hreuCENr67LTVFikabHAihBCffPKJaNOmjfD19RXx8fFi8+bNajdJADD7s3DhQiGEENnZ2WLIkCGiWbNmws/PT3Ts2FG8/PLLJnUyhBDixIkT4tZbbxUBAQEiNDRUvPjii6KystJkm7Vr14rY2Fjh6+sr2rdvbzxGbUo/R6NHjxYtW7YUvr6+olWrVmL06NHiyJEjxvuvXLkinn76adG0aVMRGBgo7r77bpGTk+MW51bbqlWrBABx6NAhk9vd8fVbu3at2ffkhAkThBBXp0e+9dZbIjw8XPj5+Ymbb7653nmfP39ejB07VjRu3FgEBQWJ5ORkUVJSYrLNrl27xKBBg4Sfn59o1aqVmDZtWr22fPvtt6Jz587C19dX9OjRQ/zyyy8m90tpi5zzO378uMW/yZq6NZmZmSIhIUEEBwcLf39/0a1bN/H3v//d5MKu5vnZOsfLly+L4cOHixYtWggfHx/Rtm1bMXHixHpBrLu+hjU+/fRTERAQIAoLC+s9Xuuvoa3rghDa+uyU0hZbdNdOnIiIiEgTGmTOCREREWkXgxMiIiLSFAYnREREpCkMToiIiEhTGJwQERGRpjA4ISIiIk1hcEJERESawuCEiIiINIXBCREREWkKgxMiIiLSFAYnREREpCkMToiIiEhT/h/Pam/Gy4cbFQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(lossi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnstd = hpreact.std(0, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.067012071609497\n",
      "val 2.106797933578491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Let's train a deeper network\n",
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2870\n",
      "  10000/ 200000: 2.3273\n",
      "  20000/ 200000: 2.1019\n",
      "  30000/ 200000: 1.9020\n",
      "  40000/ 200000: 2.1643\n",
      "  50000/ 200000: 2.1520\n",
      "  60000/ 200000: 1.7767\n",
      "  70000/ 200000: 2.1514\n",
      "  80000/ 200000: 2.3802\n",
      "  90000/ 200000: 1.9338\n",
      " 100000/ 200000: 2.3943\n",
      " 110000/ 200000: 2.1507\n",
      " 120000/ 200000: 2.1813\n",
      " 130000/ 200000: 2.0889\n",
      " 140000/ 200000: 1.8096\n",
      " 150000/ 200000: 1.9132\n",
      " 160000/ 200000: 1.9793\n",
      " 170000/ 200000: 1.9654\n",
      " 180000/ 200000: 2.2462\n",
      " 190000/ 200000: 1.9601\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  #if i >= 1000:\n",
    "    #break # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BatchNorm forward pass as a widget\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def normshow(x0):\n",
    "  \n",
    "  g = torch.Generator().manual_seed(2147483647+1)\n",
    "  x = torch.randn(5, generator=g) * 5\n",
    "  x[0] = x0 # override the 0th example with the slider\n",
    "  mu = x.mean()\n",
    "  sig = x.std()\n",
    "  y = (x - mu)/sig\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  # plot 0\n",
    "  plt.plot([-6,6], [0,0], 'k')\n",
    "  # plot the mean and std\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
    "  # plot little lines connecting input and output\n",
    "  for i in range(len(x)):\n",
    "    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
    "  # plot the input and output values\n",
    "  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
    "  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
    "  plt.xlim(-6, 6)\n",
    "  # title\n",
    "  plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
    "\n",
    "interact(normshow, x0=(-30,30,0.5));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Linear: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\n",
    "b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\n",
    "c = b @ a\n",
    "loss = torch.randn(1000, generator=g) @ c\n",
    "a.retain_grad()\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "loss.backward()\n",
    "print('a std:', a.std().item())\n",
    "print('b std:', b.std().item())\n",
    "print('c std:', c.std().item())\n",
    "print('-----')\n",
    "print('c grad std:', c.grad.std().item())\n",
    "print('a grad std:', a.grad.std().item())\n",
    "print('b grad std:', b.grad.std().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Linear + BatchNorm: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "n = 1000\n",
    "# linear layer ---\n",
    "inp = torch.randn(n, requires_grad=True, generator=g)\n",
    "w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\n",
    "x = w @ inp\n",
    "# bn layer ---\n",
    "xmean = x.mean()\n",
    "xvar = x.var()\n",
    "out = (x - xmean) / torch.sqrt(xvar + 1e-5)\n",
    "# ----\n",
    "loss = out @ torch.randn(n, generator=g)\n",
    "inp.retain_grad()\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "out.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print('inp std: ', inp.std().item())\n",
    "print('w std: ', w.std().item())\n",
    "print('x std: ', x.std().item())\n",
    "print('out std: ', out.std().item())\n",
    "print('------')\n",
    "print('out grad std: ', out.grad.std().item())\n",
    "print('x grad std: ', x.grad.std().item())\n",
    "print('w grad std: ', w.grad.std().item())\n",
    "print('inp grad std: ', inp.grad.std().item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
